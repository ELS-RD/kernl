{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: tokenizer in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: datasets in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (2.5.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (1.23.3)\r\n",
      "Requirement already satisfied: dill<0.3.6 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.3.5.1)\r\n",
      "Requirement already satisfied: aiohttp in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (3.8.3)\r\n",
      "Requirement already satisfied: packaging in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.10.0)\r\n",
      "Requirement already satisfied: pandas in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (1.5.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (2.28.1)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (2022.8.2)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (9.0.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (4.64.1)\r\n",
      "Requirement already satisfied: responses<0.19 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: multiprocess in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.70.13)\r\n",
      "Requirement already satisfied: xxhash in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\r\n",
      "Requirement already satisfied: filelock in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from pandas->datasets) (2022.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizer datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torchdynamo\n",
    "import torch\n",
    "from typing import List\n",
    "from kernl.optimizer.dynamo_backend import dynamo_backend_ofi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_name = \"BaptisteDoyen/camembert-base-xnli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nli_model = nli_model.eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "torchdynamo.config.cache_size_limit = 256"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xnli (/home/geantvert/.cache/huggingface/datasets/xnli/fr/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afdef8c7dc8f4ca38bc9631c50e8c98a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/BaptisteDoyen/camembert-base-xnli\n",
    "dataset = load_dataset('xnli', 'fr')\n",
    "\n",
    "inputs: list[torch.Tensor] = list()\n",
    "\n",
    "for index, content in enumerate(dataset['test']):\n",
    "    premise, hypothesis, label = content.values()\n",
    "    x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "    inputs.append(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete_time=50.011566400527954\n"
     ]
    }
   ],
   "source": [
    "complete_time = 0\n",
    "for index, x in enumerate(inputs):\n",
    "    x = x.to(\"cuda\")\n",
    "\n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True):\n",
    "        start = time.time()\n",
    "        _ = nli_model(**x)\n",
    "        torch.cuda.synchronize()\n",
    "        complete_time += time.time() - start\n",
    "\n",
    "print(f\"{complete_time=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from kernl.implementations.cuda_graph import cuda_graphs_wrapper\n",
    "\n",
    "nli_model2 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nli_model2 = nli_model2.eval().cuda()\n",
    "\n",
    "\n",
    "pool: (int, int) = torch.cuda.graph_pool_handle()\n",
    "def compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    dynamo_backend_ofi(gm)\n",
    "    return cuda_graphs_wrapper(gm, example_inputs, pool=pool)\n",
    "\n",
    "def run(*args, **kwargs):\n",
    "    with torchdynamo.optimize(compiler):\n",
    "        return nli_model2(*args, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we do a warmup, it builds the triton kernels optimized for each size.\n",
    "Moreover, we try to make the cuda graphs pool memory as big as possible to avoid having to rebuild it during the inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_fma took 7.927716493606567 seconds\n",
      "kernel_fma took 8.874297380447388 seconds\n",
      "kernel_fma took 4.047192096710205 seconds\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "kernel_fma took 10.551846027374268 seconds\n",
      "kernel_fma took 11.579253673553467 seconds\n",
      "kernel_fma took 10.418206930160522 seconds\n",
      "kernel_fma took 6.41706395149231 seconds\n",
      "kernel_fma took 4.612836599349976 seconds\n",
      "kernel_fma took 7.910595178604126 seconds\n",
      "kernel_fma took 6.451593637466431 seconds\n",
      "kernel_fma took 4.690919876098633 seconds\n",
      "kernel_fma took 7.18576717376709 seconds\n",
      "kernel_fma took 6.995722055435181 seconds\n",
      "kernel_fma took 4.591745615005493 seconds\n",
      "kernel_fma took 7.442533254623413 seconds\n",
      "kernel_fma took 7.224061489105225 seconds\n",
      "kernel_fma took 4.639931917190552 seconds\n",
      "kernel_fma took 5.797223091125488 seconds\n",
      "kernel_fma took 6.531261920928955 seconds\n",
      "kernel_fma took 4.650022029876709 seconds\n",
      "kernel_fma took 6.072131395339966 seconds\n",
      "kernel_fma took 5.706998348236084 seconds\n",
      "kernel_fma took 3.8703203201293945 seconds\n",
      "kernel_fma took 7.490934133529663 seconds\n",
      "kernel_fma took 6.6641552448272705 seconds\n",
      "kernel_fma took 4.172919273376465 seconds\n",
      "kernel_fma took 7.683231592178345 seconds\n",
      "kernel_fma took 5.599915504455566 seconds\n",
      "kernel_fma took 3.909942388534546 seconds\n",
      "kernel_fma took 7.77799916267395 seconds\n",
      "kernel_fma took 6.115581512451172 seconds\n",
      "kernel_fma took 4.282452821731567 seconds\n",
      "kernel_fma took 7.714815139770508 seconds\n",
      "kernel_fma took 7.015957593917847 seconds\n",
      "kernel_fma took 4.9091410636901855 seconds\n",
      "kernel_fma took 9.453371047973633 seconds\n",
      "kernel_fma took 6.882636547088623 seconds\n",
      "kernel_fma took 4.562577724456787 seconds\n",
      "kernel_fma took 7.40438985824585 seconds\n",
      "kernel_fma took 7.288777112960815 seconds\n",
      "kernel_fma took 4.619569301605225 seconds\n",
      "kernel_fma took 7.313748359680176 seconds\n",
      "kernel_fma took 7.003617286682129 seconds\n",
      "kernel_fma took 4.684171915054321 seconds\n",
      "kernel_fma took 7.504475355148315 seconds\n",
      "kernel_fma took 6.954384088516235 seconds\n",
      "kernel_fma took 4.69801664352417 seconds\n",
      "kernel_fma took 7.571547269821167 seconds\n",
      "kernel_fma took 6.972818374633789 seconds\n",
      "kernel_fma took 4.694792032241821 seconds\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "shapes = [(1, w) for w in range(8, 128+8, 8)]\n",
    "with torch.inference_mode(), torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True):\n",
    "    for _ in range(100):\n",
    "        for s in shapes:\n",
    "            # print(s)\n",
    "            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True):\n",
    "                x = {'input_ids': torch.randint(1, 10000, s, device=\"cuda\").to(torch.long),\n",
    "                   'attention_mask': torch.ones(s, device=\"cuda\", dtype=torch.long)}\n",
    "                a = run(**x)\n",
    "                torch.cuda.synchronize()\n",
    "print(\"finish\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.699 torch.Size([1, 40])\n",
      "1 5.917 torch.Size([1, 48])\n",
      "3 5.880 torch.Size([1, 80])\n",
      "4 6.103 torch.Size([1, 88])\n",
      "12 6.285 torch.Size([1, 56])\n",
      "14 6.382 torch.Size([1, 64])\n",
      "15 6.522 torch.Size([1, 24])\n",
      "19 6.614 torch.Size([1, 72])\n",
      "21 6.915 torch.Size([1, 32])\n",
      "209 6.802 torch.Size([1, 16])\n",
      "383 6.952 torch.Size([1, 96])\n",
      "count=1000\n",
      "count=2000\n",
      "count=3000\n",
      "3480 7.148 torch.Size([1, 104])\n",
      "count=4000\n",
      "4482 7.216 torch.Size([1, 112])\n",
      "4483 7.429 torch.Size([1, 128])\n",
      "4484 7.332 torch.Size([1, 120])\n",
      "count=5000\n",
      "count=5010\n",
      "complete_time=106.256\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "complete_time = 0\n",
    "count = 0\n",
    "for index, x in enumerate(inputs):\n",
    "    x = x.to(\"cuda\")\n",
    "    if x[\"input_ids\"].shape[1] > 128:\n",
    "        print(\"long input, pass\")\n",
    "        continue\n",
    "    assert x[\"input_ids\"].shape[0] == 1\n",
    "    assert x[\"input_ids\"].shape[1] <= 128\n",
    "    assert x[\"input_ids\"].shape[1] % 8 == 0\n",
    "    torch.cuda.synchronize()\n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True):\n",
    "        start = time.time()\n",
    "        _ = run(**x)\n",
    "        torch.cuda.synchronize()\n",
    "        complete_time += time.time() - start\n",
    "        if time.time() - start > 0.1:\n",
    "            print(index, f\"{time.time() - start:.3f}\", x[\"input_ids\"].shape)\n",
    "\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"{count=}\")\n",
    "print(f\"{count=}\")\n",
    "print(f\"{complete_time=:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From previous run:\n",
    "\n",
    "0 5.699 torch.Size([1, 40])\n",
    "1 5.917 torch.Size([1, 48])\n",
    "3 5.880 torch.Size([1, 80])\n",
    "4 6.103 torch.Size([1, 88])\n",
    "12 6.285 torch.Size([1, 56])\n",
    "14 6.382 torch.Size([1, 64])\n",
    "15 6.522 torch.Size([1, 24])\n",
    "19 6.614 torch.Size([1, 72])\n",
    "21 6.915 torch.Size([1, 32])\n",
    "209 6.802 torch.Size([1, 16])\n",
    "383 6.952 torch.Size([1, 96])\n",
    "count=1000\n",
    "count=2000\n",
    "count=3000\n",
    "3480 7.148 torch.Size([1, 104])\n",
    "count=4000\n",
    "4482 7.216 torch.Size([1, 112])\n",
    "4483 7.429 torch.Size([1, 128])\n",
    "4484 7.332 torch.Size([1, 120])\n",
    "count=5000\n",
    "count=5010\n",
    "complete_time=106.256\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "Personne ne savait où ils étaient. Leur destination était secrète.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][209].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96])\n",
      "Et je lui ai demandé, tu sais, est-ce que je pourrais le faire, euh, as-tu besoin que je reste et que je le fasse ce soir ou  est-ce que je peux le finir pour demain midi, si c'est d'accord. J'ai demandé au client s'il serait en colère si ça attendait jusqu'à 14 h car j'avais un rendez-vous ce soir.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][383].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 104])\n",
      "Nous avons conscience qu'il nous aurait été difficile d'argumenter en faveur d'un coûteux changement de la stratégie de défense de la NORAD (agence de défense aérospatiale nord-américaine) afin de contrer le danger d'une attaque de pirates de l'air kamikazes, avant même qu'une telle menace ait jamais eu lieu. Il en coûte plus de cinq millions de dollars par jour pour améliorer la position de défense du NORAD.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][3480].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 112])\n",
      "L'ensemble de l'œuvre de Dowd, et en particulier la série de chroniques nommée Flytrap (attrape-mouche) qui lui a valu le Pulitzer, est jusqu'ici un des exemples les plus brillants de la propension des baby-boomers à l'auto-flagellation. Dowd n'a jamais écrit et n'écrira jamais quoi que ce soit qui puisse être interprété comme de l'auto-punition de baby-boomer.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][4482].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "L'ensemble de l'œuvre de Dowd, et en particulier la série de chroniques nommée Flytrap (attrape-mouche) qui lui a valu le Pulitzer, est jusqu'ici un des exemples les plus brillants de la propension des baby-boomers à l'auto-flagellation. Pour de brillants exemples d'auto-châtiment de la part des boomers, ne cherchez pas plus loin que les pièces Flytrap gagnantes du Pulitzer de Dowd, et, en fait, la plupart de son travail en tant que chroniqueur.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][4483].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120])\n",
      "L'ensemble de l'œuvre de Dowd, et en particulier la série de chroniques nommée Flytrap (attrape-mouche) qui lui a valu le Pulitzer, est jusqu'ici un des exemples les plus brillants de la propension des baby-boomers à l'auto-flagellation. En plus de l'un des exemples les plus brillants de l'autocastigation des baby-boomers, le travail de Dowd comprend de nombreuses chroniques sur la poursuite des environnementalistes radicaux.\n"
     ]
    }
   ],
   "source": [
    "premise, hypothesis, _ = dataset['test'][4484].values()\n",
    "x = tokenizer(premise, hypothesis, return_tensors='pt', pad_to_multiple_of=8, padding=True, truncation=True)\n",
    "print(x[\"input_ids\"].shape)\n",
    "print(premise, hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.66706586826347\n",
      "128\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sizes = [len(tokenizer(d[\"premise\"], d[\"hypothesis\"], pad_to_multiple_of=8, padding=True, truncation=True)['input_ids']) for d in dataset['test']]\n",
    "print(sum(sizes) / len(sizes))\n",
    "print(max(sizes))\n",
    "print(len([1 for s in sizes if s > 100]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
