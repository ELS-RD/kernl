{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Computation of an online softmax\n",
    "\n",
    "Naive implementation of `softmax` computation requires to perform several passes on all data.\n",
    "Data reading from global memory (GPU DRAM) becomes the operation bottleneck.\n",
    "\n",
    "`softmax` `triton` tutorial assumes that whole vector is small enough to be loaded in SRAM.\n",
    "It solves data access bottleneck but doesn't tell us what to do when the vector is too large?\n",
    "\n",
    "In the case of `transformer` model, the `softmax` is applied to a matrix of shape `(sequence length, sequence length)`.\n",
    "\n",
    "In their paper [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867.pdf), *M. Milakov & Al.* show an approach which makes parallelization possible by computing softmax in small blocks and incorporating progressively new knowledge from new data. It's a key part of the solution we will develop below.\n",
    "\n",
    "FWIW, existing implementation of `online softmax`: https://github.com/jenkspt/online-softmax-jax (not tested)\n",
    "\n",
    "## The original limitations\n",
    "\n",
    "Softmax computation requires 2 elements known from whole vector:\n",
    "- denominator is the sum of the exponential of each vector element;\n",
    "- to avoid having overflow with `FP16` or `FP32` numbers, it's usual to substract the maximum value of the vector to each of its elements before applying the operation wise expentional operator.\n",
    "\n",
    "## Problem setup\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "M, N, K = 12, 4, 16  # for simplification M and K are multiples of N\n",
    "block_size_M, block_size_K = N, N\n",
    "\n",
    "long_input_vec = np.random.random((M, K))\n",
    "small_vec = np.random.random((K, N))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To avoid overflow in `softmax` computation it's usual to substract the maximul value of the vector.\n",
    "Of course, this operation has no effect on the final result mathematically.\n",
    "This is sometimes called `safe softmax` computation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "expected_softmax = softmax(long_input_vec, axis=1)\n",
    "expected_attention = expected_softmax @ small_vec\n",
    "\n",
    "input_safe = long_input_vec - np.max(long_input_vec, axis=1)[:, None]\n",
    "\n",
    "softmax_numerator = np.exp(input_safe)\n",
    "softmax_denominator = np.sum(softmax_numerator, axis=1)[:, None]\n",
    "naive_softmax = softmax_numerator / softmax_denominator\n",
    "matmul_result = naive_softmax @ small_vec\n",
    "\n",
    "assert np.allclose(naive_softmax, expected_softmax)\n",
    "assert np.allclose(matmul_result, expected_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Direct implementation of the paper without vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "online_softmax_simple = np.zeros_like(long_input_vec)\n",
    "rows, cols = long_input_vec.shape\n",
    "\n",
    "for row in range(rows):\n",
    "    max_row = 0.\n",
    "    softmax_denominator = 0.\n",
    "    for col in range(cols):\n",
    "        val = long_input_vec[row, col]\n",
    "        old_max_row = max_row\n",
    "        max_row = max(old_max_row, val)\n",
    "        softmax_denominator = softmax_denominator * np.exp(old_max_row - max_row) + np.exp(val - max_row)\n",
    "\n",
    "    for index, j in enumerate(long_input_vec[row, :]):\n",
    "        online_softmax_simple[row, index] = np.exp(j - max_row) / softmax_denominator\n",
    "\n",
    "assert np.allclose(online_softmax_simple, expected_softmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "online_softmax = np.zeros_like(long_input_vec)\n",
    "online_attention = np.zeros((M, N))\n",
    "\n",
    "for block_start_M in range(0, M, block_size_M):\n",
    "    block_end_M = block_start_M + block_size_M\n",
    "    # init some variables required at the row level\n",
    "    # line 4, mi will store the row max (computed progressively, block after block)\n",
    "    mi = np.full((block_size_M, 1), -np.inf)\n",
    "    # line 4, li will store the denominator of the softmax\n",
    "    li = np.zeros((block_size_M, 1))\n",
    "    # Oi contains the matmum result\n",
    "    Oi = online_attention[block_start_M:block_end_M, :]\n",
    "    for block_start_K in range(0, K, block_size_K):\n",
    "        block_end_K = block_start_K + block_size_K\n",
    "        block = long_input_vec[block_start_M:block_end_M, block_start_K:block_end_K]\n",
    "        # line 6\n",
    "        Vj = small_vec[block_start_K:block_end_K, :]\n",
    "        # line 10, the row max of the block\n",
    "        mij_hat = np.max(block, axis=1)[:, None]\n",
    "        # line 10\n",
    "        pij_hat = np.exp(block - mij_hat)\n",
    "        # line 10\n",
    "        lij_hat = np.sum(pij_hat, axis=1)[:, None]\n",
    "\n",
    "        # line 11, the row max so far\n",
    "        mi_new = np.max(np.column_stack([mi, mij_hat]), axis=1)[:, None]\n",
    "\n",
    "        # line 11\n",
    "        li_new = np.exp(mi - mi_new) * li + np.exp(mij_hat - mi_new) * lij_hat\n",
    "\n",
    "        # line 12\n",
    "        Oi = (li * np.exp(mi - mi_new) * Oi / li_new) + (np.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj\n",
    "        online_attention[block_start_M:block_end_M, :] = Oi\n",
    "\n",
    "        # line 13\n",
    "        mi = mi_new\n",
    "        li = li_new\n",
    "\n",
    "\n",
    "assert np.allclose(online_attention, expected_attention)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}