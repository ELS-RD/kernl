{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:08.670920Z",
     "iopub.status.busy": "2022-10-09T10:17:08.670500Z",
     "iopub.status.idle": "2022-10-09T10:17:10.879953Z",
     "shell.execute_reply": "2022-10-09T10:17:10.878323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: tokenizer in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (3.4.2)\r\n",
      "Requirement already satisfied: sentencepiece in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (0.1.97)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\r\n",
      "You should consider upgrading via the '/home/geantvert/.local/share/virtualenvs/kernl/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Sat Oct 15 21:51:59 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:03:00.0  On |                  N/A |\r\n",
      "| 56%   58C    P0    92W / 350W |    139MiB / 24576MiB |     33%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1688      G   /usr/lib/xorg/Xorg                 80MiB |\r\n",
      "|    0   N/A  N/A      7679      G   /usr/bin/gnome-shell               31MiB |\r\n",
      "|    0   N/A  N/A    470368      G   ...RendererForSitePerProcess       25MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizer sentencepiece\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:10.887206Z",
     "iopub.status.busy": "2022-10-09T10:17:10.886686Z",
     "iopub.status.idle": "2022-10-09T10:17:12.410746Z",
     "shell.execute_reply": "2022-10-09T10:17:12.409928Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import time\n",
    "import torchdynamo\n",
    "import torch\n",
    "from typing import List\n",
    "from kernl.optimizer.dynamo_backend import dynamo_backend_ofi\n",
    "from kernl.implementations.cuda_graph import cuda_graphs_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default cache size needs to be increased to store the many graphs with generative models\n",
    "torchdynamo.config.cache_size_limit = 512\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"Translate in English: c'est beaucoup plus rapide avec ces optimisations !\",\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    ").to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482590675354004\n",
      "c'est beaucoup plus rapide avec ces optimisations!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    for _ in range(3):\n",
    "        output = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=100,\n",
    "            max_length=100,\n",
    "        )\n",
    "    start = time.time()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:19.111208Z",
     "iopub.status.busy": "2022-10-09T10:17:19.110747Z",
     "iopub.status.idle": "2022-10-09T10:17:19.117920Z",
     "shell.execute_reply": "2022-10-09T10:17:19.116830Z"
    }
   },
   "outputs": [],
   "source": [
    "model.encoder.forward2 = model.encoder.forward\n",
    "model.decoder.forward2 = model.decoder.forward\n",
    "\n",
    "# share cuda pool among all cuda graphs\n",
    "pool: (int, int) = torch.cuda.graph_pool_handle()\n",
    "\n",
    "\n",
    "def compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    dynamo_backend_ofi(gm)\n",
    "    return cuda_graphs_wrapper(gm, example_inputs, pool=pool)\n",
    "\n",
    "\n",
    "def run_encoder(*args, **kwargs):\n",
    "    with torchdynamo.optimize(compiler):\n",
    "        return model.encoder.forward2(*args, **kwargs)\n",
    "\n",
    "\n",
    "def run_decoder(*args, **kwargs):\n",
    "    with torchdynamo.optimize(compiler):\n",
    "        return model.decoder.forward2(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    out1 = run_decoder(**input_ids)\n",
    "    out2 = model.decoder(**input_ids)\n",
    "    assert torch.allclose(out1.last_hidden_state, out2.last_hidden_state, atol=1e-1)\n",
    "    out3 = run_encoder(**input_ids)\n",
    "    out4 = model.encoder(**input_ids)\n",
    "    assert torch.allclose(out3.last_hidden_state, out4.last_hidden_state, atol=1e-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model.encoder.forward = run_encoder\n",
    "model.decoder.forward = run_decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "TORCHDYNAMO: backend compiler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py\", line 362, in call_user_compiler\n",
      "    compiled_fn = self.compiler_fn(gm, self.example_inputs())\n",
      "  File \"/tmp/ipykernel_731904/2186925895.py\", line 9, in compiler\n",
      "    return cuda_graphs_wrapper(gm, example_inputs, pool=pool)\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/cuda_graph.py\", line 40, in cuda_graphs_wrapper\n",
      "    model(*inputs)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 652, in call_wrapped\n",
      "    return self._wrapped_call(self, *args, **kwargs)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 277, in __call__\n",
      "    raise e\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 267, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<eval_with_key>.269\", line 100, in forward\n",
      "    attention_wrapper_1 = kernl_optimizer_attention_attention_wrapper(transpose_5, transpose_6, transpose_7, empty_like_1, 1.0, False, add_6);  transpose_5 = empty_like_1 = None\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/optimizer/attention.py\", line 23, in attention_wrapper\n",
      "    return attention_forward(q, k, v, output, sm_scale, is_causal=is_causal, attention_mask=attention_mask)\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/attention.py\", line 436, in attention_forward\n",
      "    return Attention.apply(q, k, v, output, sm_scale, is_causal, attention_mask)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\", line 116, in decorate_fwd\n",
      "    return fwd(*_cast(args, cast_inputs), **_cast(kwargs, cast_inputs))\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/attention.py\", line 376, in forward\n",
      "    assert attention_mask.size(3) == seq_length, f\"Last size of mask must be seq_length to broadcast on QK^t: {attention_mask.size(3)} != {seq_length}\"\n",
      "AssertionError: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1\n",
      "----------------------------------------\n",
      "ERROR:root:Error while processing frame\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py\", line 362, in call_user_compiler\n",
      "    compiled_fn = self.compiler_fn(gm, self.example_inputs())\n",
      "  File \"/tmp/ipykernel_731904/2186925895.py\", line 9, in compiler\n",
      "    return cuda_graphs_wrapper(gm, example_inputs, pool=pool)\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/cuda_graph.py\", line 40, in cuda_graphs_wrapper\n",
      "    model(*inputs)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 652, in call_wrapped\n",
      "    return self._wrapped_call(self, *args, **kwargs)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 277, in __call__\n",
      "    raise e\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 267, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<eval_with_key>.269\", line 100, in forward\n",
      "    attention_wrapper_1 = kernl_optimizer_attention_attention_wrapper(transpose_5, transpose_6, transpose_7, empty_like_1, 1.0, False, add_6);  transpose_5 = empty_like_1 = None\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/optimizer/attention.py\", line 23, in attention_wrapper\n",
      "    return attention_forward(q, k, v, output, sm_scale, is_causal=is_causal, attention_mask=attention_mask)\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/attention.py\", line 436, in attention_forward\n",
      "    return Attention.apply(q, k, v, output, sm_scale, is_causal, attention_mask)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\", line 116, in decorate_fwd\n",
      "    return fwd(*_cast(args, cast_inputs), **_cast(kwargs, cast_inputs))\n",
      "  File \"/home/geantvert/workspace/kernl/src/kernl/implementations/attention.py\", line 376, in forward\n",
      "    assert attention_mask.size(3) == seq_length, f\"Last size of mask must be seq_length to broadcast on QK^t: {attention_mask.size(3)} != {seq_length}\"\n",
      "AssertionError: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/eval_frame.py\", line 148, in catch_errors\n",
      "    return callback(frame, cache_size)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py\", line 347, in _convert_frame\n",
      "    result = inner_convert(frame, cache_size)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py\", line 108, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py\", line 288, in _convert_frame_assert\n",
      "    code = transform_code_object(frame.f_code, transform)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/bytecode_transformation.py\", line 338, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py\", line 264, in transform\n",
      "    tracer.run()\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py\", line 312, in run\n",
      "    and self.step()\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py\", line 290, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py\", line 1335, in RETURN_VALUE\n",
      "    self.output.compile_subgraph(self)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py\", line 307, in compile_subgraph\n",
      "    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py\", line 348, in compile_and_call_fx_graph\n",
      "    compiled_fn = self.call_user_compiler(gm)\n",
      "  File \"/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py\", line 371, in call_user_compiler\n",
      "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "torchdynamo.exc.BackendCompilerFailed: compiler raised AssertionError: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    torchdynamo.config.raise_on_backend_error = False\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "compiler raised AssertionError: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1\n\nYou can suppress this exception and fall back to eager by setting:\n    torchdynamo.config.raise_on_backend_error = False",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py:362\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 362\u001B[0m     compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexample_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m callable(compiled_fn), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiler_fn did not return callable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn [6], line 9\u001B[0m, in \u001B[0;36mcompiler\u001B[0;34m(gm, example_inputs)\u001B[0m\n\u001B[1;32m      8\u001B[0m dynamo_backend_ofi(gm)\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcuda_graphs_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpool\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/kernl/src/kernl/implementations/cuda_graph.py:40\u001B[0m, in \u001B[0;36mcuda_graphs_wrapper\u001B[0;34m(model, inputs, copy_outputs, pool)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m---> 40\u001B[0m         \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m stream\u001B[38;5;241m.\u001B[39msynchronize()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py:652\u001B[0m, in \u001B[0;36mGraphModule.recompile.<locals>.call_wrapped\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_wrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrapped_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py:277\u001B[0m, in \u001B[0;36m_WrappedCall.__call__\u001B[0;34m(self, obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001B[0m, in \u001B[0;36m_WrappedCall.__call__\u001B[0;34m(self, obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 267\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
      "File \u001B[0;32m<eval_with_key>.269:100\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(self, input_ids, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[1;32m     99\u001B[0m empty_like_1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty_like(transpose_5)\n\u001B[0;32m--> 100\u001B[0m attention_wrapper_1 \u001B[38;5;241m=\u001B[39m \u001B[43mkernl_optimizer_attention_attention_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranspose_5\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtranspose_6\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtranspose_7\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mempty_like_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_6\u001B[49m\u001B[43m)\u001B[49m;  transpose_5 \u001B[38;5;241m=\u001B[39m empty_like_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    101\u001B[0m transpose_9 \u001B[38;5;241m=\u001B[39m attention_wrapper_1\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m);  attention_wrapper_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/kernl/src/kernl/optimizer/attention.py:23\u001B[0m, in \u001B[0;36mattention_wrapper\u001B[0;34m(q, k, v, output, sm_scale, is_causal, attention_mask)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattention_wrapper\u001B[39m(q, k, v, output, sm_scale, is_causal, attention_mask):\n\u001B[0;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mattention_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msm_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/kernl/src/kernl/implementations/attention.py:436\u001B[0m, in \u001B[0;36mattention_forward\u001B[0;34m(q, k, v, output, sm_scale, is_causal, attention_mask)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattention_forward\u001B[39m(\n\u001B[1;32m    428\u001B[0m     q: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m    429\u001B[0m     k: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    434\u001B[0m     attention_mask: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    435\u001B[0m ):\n\u001B[0;32m--> 436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mAttention\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msm_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:116\u001B[0m, in \u001B[0;36mcustom_fwd.<locals>.decorate_fwd\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast(enabled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfwd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_cast\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcast_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_cast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcast_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/workspace/kernl/src/kernl/implementations/attention.py:376\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(ctx, q, k, v, output, sm_scale, is_causal, attention_mask)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    374\u001B[0m     attention_mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m==\u001B[39m seq_length \u001B[38;5;129;01mor\u001B[39;00m attention_mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    375\u001B[0m ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncompatible broadcast seq_length dimension\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 376\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m attention_mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m3\u001B[39m) \u001B[38;5;241m==\u001B[39m seq_length, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLast size of mask must be seq_length to broadcast on QK^t: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattention_mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseq_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    378\u001B[0m HAS_MASK \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode(), torch\u001B[38;5;241m.\u001B[39mautocast(dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16, cache_enabled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, device_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m      3\u001B[0m     start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/transformers/generation_utils.py:1390\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1386\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_return_sequences has to be 1, but is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_return_sequences\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m when doing greedy search.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1387\u001B[0m         )\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;66;03m# 10. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1391\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1392\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1393\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1394\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1395\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1397\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1398\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1399\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1400\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_sample_gen_mode:\n\u001B[1;32m   1403\u001B[0m     \u001B[38;5;66;03m# 10. prepare logits warper\u001B[39;00m\n\u001B[1;32m   1404\u001B[0m     logits_warper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(\n\u001B[1;32m   1405\u001B[0m         top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1406\u001B[0m         top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1410\u001B[0m         renormalize_logits\u001B[38;5;241m=\u001B[39mrenormalize_logits,\n\u001B[1;32m   1411\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/transformers/generation_utils.py:1785\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1782\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   1784\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 1785\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1786\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1788\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1789\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1790\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1792\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   1793\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1648\u001B[0m, in \u001B[0;36mT5ForConditionalGeneration.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1645\u001B[0m         decoder_attention_mask \u001B[38;5;241m=\u001B[39m decoder_attention_mask\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mfirst_device)\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[0;32m-> 1648\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1650\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1651\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1652\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1653\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1654\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1655\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1656\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1657\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1658\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1659\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1660\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1661\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1663\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m decoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1665\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn [6], line 19\u001B[0m, in \u001B[0;36mrun_decoder\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_decoder\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torchdynamo\u001B[38;5;241m.\u001B[39moptimize(compiler):\n\u001B[0;32m---> 19\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward2\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/eval_frame.py:148\u001B[0m, in \u001B[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001B[0;34m(frame, cache_size)\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m compile_lock:\n\u001B[0;32m--> 148\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    150\u001B[0m     logging\u001B[38;5;241m.\u001B[39mbasicConfig()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py:347\u001B[0m, in \u001B[0;36mconvert_frame.<locals>._convert_frame\u001B[0;34m(frame, cache_size)\u001B[0m\n\u001B[1;32m    345\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 347\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43minner_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    348\u001B[0m     counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py:108\u001B[0m, in \u001B[0;36mwrap_convert_context.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    106\u001B[0m torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mgraph_module\u001B[38;5;241m.\u001B[39m_forward_from_src \u001B[38;5;241m=\u001B[39m fx_forward_from_src_skip_result\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    110\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py:288\u001B[0m, in \u001B[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001B[0;34m(frame, cache_size)\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mcount():\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 288\u001B[0m         code \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_code_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m         orig_code_map[code] \u001B[38;5;241m=\u001B[39m frame\u001B[38;5;241m.\u001B[39mf_code\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/bytecode_transformation.py:338\u001B[0m, in \u001B[0;36mtransform_code_object\u001B[0;34m(code, transformations, safe)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(code_options[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mco_varnames\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m==\u001B[39m code_options[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mco_nlocals\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    336\u001B[0m instructions \u001B[38;5;241m=\u001B[39m cleaned_instructions(code, safe)\n\u001B[0;32m--> 338\u001B[0m \u001B[43mtransformations\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    340\u001B[0m fix_vars(instructions, code_options)\n\u001B[1;32m    342\u001B[0m dirty \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/convert_frame.py:264\u001B[0m, in \u001B[0;36mconvert_frame_assert.<locals>._convert_frame_assert.<locals>.transform\u001B[0;34m(instructions, code_options)\u001B[0m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01mnonlocal\u001B[39;00m output\n\u001B[1;32m    254\u001B[0m tracer \u001B[38;5;241m=\u001B[39m InstructionTranslator(\n\u001B[1;32m    255\u001B[0m     instructions,\n\u001B[1;32m    256\u001B[0m     frame\u001B[38;5;241m.\u001B[39mf_code,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    262\u001B[0m     one_graph,\n\u001B[1;32m    263\u001B[0m )\n\u001B[0;32m--> 264\u001B[0m \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m output \u001B[38;5;241m=\u001B[39m tracer\u001B[38;5;241m.\u001B[39moutput\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m output\u001B[38;5;241m.\u001B[39moutput_instructions\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py:312\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    308\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    309\u001B[0m         \u001B[38;5;28;01mwhile\u001B[39;00m (\n\u001B[1;32m    310\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstruction_pointer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    311\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mshould_exit\n\u001B[0;32m--> 312\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         ):\n\u001B[1;32m    314\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    316\u001B[0m         exc\u001B[38;5;241m.\u001B[39mBackendCompilerFailed,\n\u001B[1;32m    317\u001B[0m         exc\u001B[38;5;241m.\u001B[39mRestartAnalysis,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    320\u001B[0m         exc\u001B[38;5;241m.\u001B[39mUnsupported,\n\u001B[1;32m    321\u001B[0m     ):\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py:290\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, inst\u001B[38;5;241m.\u001B[39mopname):\n\u001B[1;32m    289\u001B[0m         unimplemented(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minst\u001B[38;5;241m.\u001B[39mopname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 290\u001B[0m     \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inst\u001B[38;5;241m.\u001B[39mopname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/symbolic_convert.py:1335\u001B[0m, in \u001B[0;36mInstructionTranslator.RETURN_VALUE\u001B[0;34m(self, inst)\u001B[0m\n\u001B[1;32m   1333\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mSkipFrame()\n\u001B[1;32m   1334\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstruction_pointer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1335\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_subgraph\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39madd_output_instructions([create_instruction(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m)])\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py:307\u001B[0m, in \u001B[0;36mOutputGraph.compile_subgraph\u001B[0;34m(self, tx, partial_convert)\u001B[0m\n\u001B[1;32m    304\u001B[0m output \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count_calls(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    306\u001B[0m     output\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m--> 307\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_and_call_fx_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpass2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_output_vars\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[1;32m    310\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    311\u001B[0m         output\u001B[38;5;241m.\u001B[39mappend(pass2\u001B[38;5;241m.\u001B[39mcreate_store(graph_output_var))\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py:348\u001B[0m, in \u001B[0;36mOutputGraph.compile_and_call_fx_graph\u001B[0;34m(self, tx, rv, root)\u001B[0m\n\u001B[1;32m    346\u001B[0m gm\u001B[38;5;241m.\u001B[39mrecompile()\n\u001B[1;32m    347\u001B[0m name \u001B[38;5;241m=\u001B[39m unique_id(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__compiled_fn\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 348\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_user_compiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m torchdynamo\u001B[38;5;241m.\u001B[39mdisable(compiled_fn)\n\u001B[1;32m    350\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstats\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munique_graphs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/torchdynamo/output_graph.py:371\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m    369\u001B[0m     compiled_fn \u001B[38;5;241m=\u001B[39m gm\u001B[38;5;241m.\u001B[39mforward\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mraise_on_backend_error:\n\u001B[0;32m--> 371\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m BackendCompilerFailed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiler_fn, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_fn\n",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m: compiler raised AssertionError: Last size of mask must be seq_length to broadcast on QK^t: 24 != 1\n\nYou can suppress this exception and fall back to eager by setting:\n    torchdynamo.config.raise_on_backend_error = False"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "047edc3d04374c528f27024bb17a654b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a40339901e3d4b4ba40b54d047ba8ea9",
       "placeholder": "​",
       "style": "IPY_MODEL_6c2241efc65345ffa6b32847eec24ae9",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "61727d1224ca498fa3d81b8456047c8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c2241efc65345ffa6b32847eec24ae9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ffd37b7da934584a97f27a143531df6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a40339901e3d4b4ba40b54d047ba8ea9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab31f1f54ef9429783d03a05e906517e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_047edc3d04374c528f27024bb17a654b",
        "IPY_MODEL_de91c477767649a59ab0df6e3255ff61",
        "IPY_MODEL_b84e493dfa184888b745c8cafd2a40aa"
       ],
       "layout": "IPY_MODEL_61727d1224ca498fa3d81b8456047c8d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ad7ce67b0f1a4c0683d3635fdac61575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b34763a9b06a4e178f5cea545f118988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b84e493dfa184888b745c8cafd2a40aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ad7ce67b0f1a4c0683d3635fdac61575",
       "placeholder": "​",
       "style": "IPY_MODEL_f44c8b256f0747f9b277d56f39049264",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:00&lt;00:00, 73.34it/s]"
      }
     },
     "de91c477767649a59ab0df6e3255ff61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b34763a9b06a4e178f5cea545f118988",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ffd37b7da934584a97f27a143531df6",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "f44c8b256f0747f9b277d56f39049264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
