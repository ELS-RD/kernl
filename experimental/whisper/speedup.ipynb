{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Whisper benchmark\n",
    "\n",
    "To measure the speedup of `Kernl` on Whisper model, we use `eval` set from `librispeech` on a 3090 RTX GPU.\n",
    "Following [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) paper we use the following setup:\n",
    "- `openai/whisper-large-v2` flavor (we use v2 of the weights)\n",
    "- beam search with 5 beams\n",
    "- only apply optimization to the decoder, because encoder counts for very little in the end-to-end latency\n",
    "- we leverage CUDA graphs to remove most of the CPU overhead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T18:24:30.924738Z",
     "iopub.status.busy": "2023-01-25T18:24:30.924551Z",
     "iopub.status.idle": "2023-01-25T18:24:33.400421Z",
     "shell.execute_reply": "2023-01-25T18:24:33.398869Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: datasets in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: soundfile in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: librosa in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.11.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (6.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (2.28.2)\r\n",
      "Requirement already satisfied: dill<0.3.7 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.3.6)\r\n",
      "Requirement already satisfied: aiohttp in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (3.8.3)\r\n",
      "Requirement already satisfied: packaging in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (23.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (4.64.1)\r\n",
      "Requirement already satisfied: xxhash in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (3.2.0)\r\n",
      "Requirement already satisfied: responses<0.19 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (1.23.5)\r\n",
      "Requirement already satisfied: multiprocess in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (0.70.14)\r\n",
      "Requirement already satisfied: pandas in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (1.5.3)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (2023.1.0)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from datasets) (10.0.1)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from soundfile) (1.15.1)\r\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (0.56.4)\r\n",
      "Requirement already satisfied: decorator>=4.0.10 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (5.1.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (1.2.1)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (1.10.0)\r\n",
      "Requirement already satisfied: pooch>=1.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (1.6.0)\r\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (0.4.2)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (3.0.0)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from librosa) (1.2.0)\r\n",
      "Requirement already satisfied: pycparser in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from cffi>=1.0->soundfile) (2.21)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: filelock in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\r\n",
      "Requirement already satisfied: setuptools in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from numba>=0.45.1->librosa) (65.5.1)\r\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from numba>=0.45.1->librosa) (0.39.1)\r\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from pooch>=1.0->librosa) (1.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T18:24:33.407253Z",
     "iopub.status.busy": "2023-01-25T18:24:33.406737Z",
     "iopub.status.idle": "2023-01-25T18:24:33.580596Z",
     "shell.execute_reply": "2023-01-25T18:24:33.579044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 25 19:24:33 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:03:00.0  On |                  N/A |\r\n",
      "| 40%   43C    P8    27W / 350W |     49MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A    245681      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T18:24:33.587272Z",
     "iopub.status.busy": "2023-01-25T18:24:33.586756Z",
     "iopub.status.idle": "2023-01-25T18:24:36.389134Z",
     "shell.execute_reply": "2023-01-25T18:24:36.388420Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "from kernl.model_optimization import optimize_model\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# torchdynamo.config.cache_size_limit = 512\n",
    "# torchdynamo.config.dynamic_shapes = True\n",
    "max_len = 50\n",
    "num_beams = 5\n",
    "model_name = \"openai/whisper-large-v2\"  # \"openai/whisper-tiny\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load data & model\n",
    "\n",
    "We set a simple function to extract tokens from audios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T18:24:36.394231Z",
     "iopub.status.busy": "2023-01-25T18:24:36.393965Z",
     "iopub.status.idle": "2023-01-25T18:24:55.861642Z",
     "shell.execute_reply": "2023-01-25T18:24:55.860630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr (/home/geantvert/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/cff5df6e7955c80a67f80e27e7e655de71c689e2d2364bece785b972acb37fe7)\n"
     ]
    }
   ],
   "source": [
    "# audio_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")  # small dataset for tests\n",
    "audio_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "\n",
    "\n",
    "def get_tokens(item: dict[str, dict]) -> torch.Tensor:\n",
    "    tensor = processor(item[\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=16_000).input_features\n",
    "    return tensor.cuda()\n",
    "\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "inputs_warmup = get_tokens(audio_dataset[0])\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(\"cuda\").eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Baseline\n",
    "\n",
    "Measures is done on mixed precision `FP16` model.\n",
    "We save each model output so we can check the quality impact of the optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T18:24:55.866476Z",
     "iopub.status.busy": "2023-01-25T18:24:55.866292Z",
     "iopub.status.idle": "2023-01-25T19:15:55.367549Z",
     "shell.execute_reply": "2023-01-25T19:15:55.366937Z"
    }
   },
   "outputs": [],
   "source": [
    "timings_original = list()\n",
    "transcriptions = list()\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    # warmup\n",
    "    model.generate(inputs_warmup, min_length=max_len, max_length=max_len, num_beams=num_beams, do_sample=False)\n",
    "    torch.cuda.synchronize()\n",
    "    for audio in audio_dataset:\n",
    "        inputs = get_tokens(audio)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        predicted_ids = model.generate(inputs, min_length=1, max_length=max_len, num_beams=num_beams, do_sample=False)\n",
    "        torch.cuda.synchronize()\n",
    "        timings_original.append(time.time() - start)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)[0]\n",
    "        transcriptions.append(transcription)\n",
    "\n",
    "assert len(audio_dataset) == len(transcriptions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optimized model\n",
    "\n",
    "### Hugging Face implementation\n",
    "\n",
    "First, we fix a small inefficiency in the Hugging Face library.  \n",
    "Basically, it avoids unnecessary encoder tensor (from K/V cache) copies.\n",
    "\n",
    "The impact on speed inference is limited, it's mainly done for memory footprint.  \n",
    "In the past, PyTorch 2.0 nightlies had many memory leaks and this fix was back then mandatory to not get OOM.  \n",
    "FWIW, all memory leaks we have found during our experiments have been fixed in recent PyTorch versions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T19:15:55.370296Z",
     "iopub.status.busy": "2023-01-25T19:15:55.370105Z",
     "iopub.status.idle": "2023-01-25T19:15:55.374512Z",
     "shell.execute_reply": "2023-01-25T19:15:55.373773Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply efficiency fix to HuggingFace implementation of Whisper to limit memory footprint\n",
    "@staticmethod\n",
    "def fix_reorder_cache(past, beam_idx):\n",
    "    reordered_past = ()\n",
    "    for layer_past in past:\n",
    "        reordered_past += (\n",
    "            tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "        )\n",
    "    return reordered_past\n",
    "\n",
    "\n",
    "WhisperForConditionalGeneration._reorder_cache = fix_reorder_cache"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Kernl optimization\n",
    "\n",
    "Warmup takes around 12 minutes and is mostly spent by PyTorch 2.0 dynamo module on CPU to capture graph (it is down from 50 min with previous version of Kernl 🤯).\n",
    "We plan to support dynamic shape mode on dynamo, preliminary benchmarks show a 5X faster warmup on Whisper large (basically mostly Triton autotune remains).\n",
    "\n",
    "Note that < 2% outputs are different from the original model.\n",
    "Manual inspection shows that differences are mostly small, many being only one different token in the whole transcription.\n",
    "\n",
    "The explanation is that our optimized kernels are mathematically equivalent to the one of the original model but may not perform operations in the same order as PyTorch kernels. As operations on float tensors always lead to rounding, order matters even for commutative operations.\n",
    "\n",
    "Moreover, our fused kernels perform accumulations in fp32 which is not possible when you chain PyTorch kernels with fp16 tensors.\n",
    "When 2 tokens have very similar scores, these rounding differences matters and that's why, in a few cases, outputs are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T19:15:55.378868Z",
     "iopub.status.busy": "2023-01-25T19:15:55.378673Z",
     "iopub.status.idle": "2023-01-25T19:49:34.590462Z",
     "shell.execute_reply": "2023-01-25T19:49:34.589828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to warmup: 11.58min\n",
      "timings\n",
      "[original] average: 1.12s / complete: 48.72min\n",
      "[optimized] average: 0.46s / complete: 19.87min\n",
      "# different outputs: 34/2620 (1.30%)\n",
      "\n",
      "memory footprint\n",
      "torch.cuda.memory_allocated: 10.9GB\n",
      "torch.cuda.memory_reserved: 13.4GB\n",
      "torch.cuda.max_memory_reserved: 13.9GB\n"
     ]
    }
   ],
   "source": [
    "optimize_model(model.model.decoder)\n",
    "nb_diff = 0\n",
    "timings_optimized = list()\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    model.generate(inputs_warmup, min_length=max_len, max_length=max_len, num_beams=num_beams, do_sample=False)\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"time to warmup: {(time.time() - start)/60:.2f}min\")\n",
    "    for original_modem_transcription, audio in zip(transcriptions, audio_dataset):\n",
    "        inputs = get_tokens(audio)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        predicted_ids = model.generate(inputs, min_length=1, max_length=max_len, num_beams=num_beams, do_sample=False)\n",
    "        torch.cuda.synchronize()\n",
    "        timings_optimized.append(time.time() - start)\n",
    "        optimized_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)[0]\n",
    "        nb_diff += original_modem_transcription != optimized_transcription\n",
    "\n",
    "print(\"timings\")\n",
    "print(\n",
    "    f\"[original] average: {sum(timings_original) / len(timings_original):.2f}s / complete: {sum(timings_original)/60:.2f}min\"\n",
    ")\n",
    "print(\n",
    "    f\"[optimized] average: {sum(timings_optimized) / len(timings_optimized):.2f}s / complete: {sum(timings_optimized)/60:.2f}min\"\n",
    ")\n",
    "print(f\"# different outputs: {nb_diff}/{len(audio_dataset)} ({nb_diff / len(audio_dataset) * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nmemory footprint\")\n",
    "print(f\"torch.cuda.memory_allocated: {torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024:.1f}GB\")\n",
    "print(f\"torch.cuda.memory_reserved: {torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024:.1f}GB\")\n",
    "print(f\"torch.cuda.max_memory_reserved: {torch.cuda.max_memory_reserved(0) / 1024 / 1024 / 1024:.1f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
