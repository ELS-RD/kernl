{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Computation of an online softmax\n",
    "\n",
    "Naive implementation of `softmax` computation requires to perform several passes on the whole input vector.\n",
    "Vector loading from global memory (GPU DRAM) from each step is by far the operation bottleneck.\n",
    "\n",
    "`softmax` `triton` tutorial assumes that the whole vector is small enough to be loaded in shared memory (`SRAM`).\n",
    "That assumption fixes the issue but do not help us when it doesn't stand, aka what to do when the vector is too large for the `SRAM`?\n",
    "\n",
    "In the case of `transformer` model, the `softmax` is applied to each row of a matrix of shape `(sequence length, sequence length)`.\n",
    "\n",
    "We will start with a naive approach and optimize it until we reach the flash attention approach.\n",
    "\n",
    "\n",
    "## Problem setup\n",
    "\n",
    "We name axis as in GEMM: M, N and K.\n",
    "For more information: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "M, N, K = 12, 4, 16  # for simplification M and K are multiples of N\n",
    "block_size_M, block_size_K = N, N\n",
    "\n",
    "long_input_vec = np.random.random((M, K))\n",
    "small_vec = np.random.random((K, N))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax computation\n",
    "\n",
    "### Safe softmax\n",
    "\n",
    "To avoid `FP16` or `FP32` overflow in `softmax` computation, it's usual to subtract to input vector its maximum value.\n",
    "This operation has no effect on the final output outside numerical stability.\n",
    "This is sometimes called `safe softmax` computation.\n",
    "\n",
    "### Memory bottleneck\n",
    "\n",
    "Computation of `safe softmax` on `PyTorch` requires multiple passes on the whole input vector if done manually:\n",
    "\n",
    "* one pass to find the maximum value\n",
    "* one pass to apply exponential operation to each value (numerator) and sum them (denominator)\n",
    "* one pass to perform the division `numerator / denominator`\n",
    "\n",
    "> because of the eager execution model, on `PyTorch` step 2 requires 2 passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expected_softmax = softmax(long_input_vec, axis=1)\n",
    "expected_attention = expected_softmax @ small_vec\n",
    "\n",
    "# 1st read\n",
    "row_max = np.max(long_input_vec, axis=1)[:, None]\n",
    "# 2nd read\n",
    "input_safe = long_input_vec - row_max\n",
    "softmax_numerator = np.exp(input_safe)\n",
    "# 3rd read\n",
    "softmax_denominator = np.sum(softmax_numerator, axis=1)[:, None]\n",
    "# 4th read\n",
    "naive_softmax = softmax_numerator / softmax_denominator\n",
    "# final matmul (another read / write)\n",
    "matmul_result = naive_softmax @ small_vec\n",
    "\n",
    "assert np.allclose(naive_softmax, expected_softmax)\n",
    "assert np.allclose(matmul_result, expected_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Online softmax\n",
    "\n",
    "In their paper [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867.pdf), *M. Milakov & Al.* show an approach which makes parallelization possible by computing `softmax` progressively.\n",
    "Basically, we load the input vector in small blocks (adapted to the size of the `SRAM`) and compute 2 statistics in a single pass:\n",
    "\n",
    "* the maximum value\n",
    "* the denominator\n",
    "\n",
    "The achievement lies in the fact that you are supposed to know the maximum value of the vector to compute the denominator.\n",
    "At each step, our knowledge of the maximum value may evolve (we may meet a value bigger than our precedent maximum).\n",
    "When it happens, we just adjust the result of our computation of the precedent step.\n",
    "\n",
    "The adjustment procedure is based on rules of exponentiation: when multiplying a base raised to one exponent by the same base raised to another exponent, the exponents add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "online_softmax_simple = np.zeros_like(long_input_vec)\n",
    "rows, cols = long_input_vec.shape\n",
    "\n",
    "for row in range(rows):\n",
    "    max_row = 0.\n",
    "    softmax_denominator = 0.\n",
    "    for col in range(cols):\n",
    "        val = long_input_vec[row, col]\n",
    "        old_max_row = max_row\n",
    "        max_row = max(old_max_row, val)\n",
    "        # np.exp(old_max_row - max_row) is the adjustment factor of our precedent softmax_denominator,\n",
    "        # after this multiplication it's like we had substracted max_row to all values instead of old_max_row\n",
    "        softmax_denominator = softmax_denominator * np.exp(old_max_row - max_row) + np.exp(val - max_row)\n",
    "\n",
    "    # leverage our 2 statistics\n",
    "    online_softmax_simple[row, :] = np.exp(long_input_vec[row, :] - max_row) / softmax_denominator\n",
    "\n",
    "assert np.allclose(online_softmax_simple, expected_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Flash attention trick\n",
    "\n",
    "`Online softmax` limits the computation to 2 passes.\n",
    "In [`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`](https://arxiv.org/pdf/2205.14135.pdf), there is only one pass, the `softmax` itself is never materialized, which is the key to the low memory footprint.\n",
    "\n",
    "The trick is that in attention mechanism, the `softmax` is followed by a `matmul` with the `V` matrix.\n",
    "In `V` matrix, the number of columns is low (<= 128), and so the output will have few columns too.\n",
    "\n",
    "Therefore, we do not need to store the whole softmax in memory, we can progressively compute it, do the `matmul`, and adjust the output of the `matmul` (multiplication being associative and commutative).\n",
    "\n",
    "Because we can leverage the same approach as explained in the chained matmul tutorial: we keep the output of the `matmul` in `SRAM`, avoiding a read / write from global memory for each block. That's the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "online_softmax = np.zeros_like(long_input_vec)\n",
    "online_attention = np.zeros((M, N))\n",
    "\n",
    "for block_start_M in range(0, M, block_size_M):\n",
    "    block_end_M = block_start_M + block_size_M\n",
    "    # init some variables required at the row level\n",
    "    # line 4, mi will store the row max (computed progressively, block after block)\n",
    "    mi = np.full((block_size_M, 1), -np.inf)\n",
    "    # line 4, li will store the denominator of the softmax\n",
    "    li = np.zeros((block_size_M, 1))\n",
    "    # load from global memory, Oi contains the matmum result\n",
    "    Oi = online_attention[block_start_M:block_end_M, :]\n",
    "    for block_start_K in range(0, K, block_size_K):\n",
    "        block_end_K = block_start_K + block_size_K\n",
    "        # load a block from input tensor\n",
    "        block = long_input_vec[block_start_M:block_end_M, block_start_K:block_end_K]\n",
    "        # line 6, load a block from matmul input tensor\n",
    "        Vj = small_vec[block_start_K:block_end_K, :]\n",
    "        # line 10, find row max of the block (and only the block)\n",
    "        mij_hat = np.max(block, axis=1)[:, None]\n",
    "        # line 10, compute the softmax numerator like if we only had the data from this block (and nothing before and after)\n",
    "        pij_hat = np.exp(block - mij_hat)\n",
    "        # line 10, compute the denominator like if we only had the data from this block (and nothing before and after)\n",
    "        lij_hat = np.sum(pij_hat, axis=1)[:, None]\n",
    "\n",
    "        # line 11, find row max regarding the current block and all the previous ones we have visited\n",
    "        mi_new = np.max(np.column_stack([mi, mij_hat]), axis=1)[:, None]\n",
    "\n",
    "        # line 11, adjusting factor leveraging the rule of exponentiation\n",
    "        li_new = np.exp(mi - mi_new) * li + np.exp(mij_hat - mi_new) * lij_hat\n",
    "\n",
    "        # line 12, first part before the \"+\" is the adjustment of the past blocks\n",
    "        # second part after the plus is the incorporation of the information from the current block and the matmul (done in steps)\n",
    "        Oi = (li * np.exp(mi - mi_new) * Oi / li_new) + (np.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj\n",
    "\n",
    "        # line 13\n",
    "        mi = mi_new\n",
    "        li = li_new\n",
    "    # save to global memory\n",
    "    online_attention[block_start_M:block_end_M, :] = Oi\n",
    "\n",
    "\n",
    "assert np.allclose(online_attention, expected_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}