{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Computation of an online softmax\n",
    "\n",
    "Softmax computation requires to have whole vector in memory to perform computation.\n",
    "This makes parallelization difficult with limited SRAM memory.\n",
    "\n",
    "In their paper [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867.pdf), M. Milakov & Al. show an approach which makes parallelization possible by computing softmax in small blocks and incorporating progressively new knowledge from new data.\n",
    "\n",
    "Existing implementation: https://github.com/jenkspt/online-softmax-jax (not tested)\n",
    "\n",
    "## The original limitations\n",
    "\n",
    "Softmax computation requires 2 elements known from whole vector:\n",
    "- denominator is the sum of the exponential of each vector element;\n",
    "- to avoid having overflow with `FP16` or `FP32` numbers, it's usual to substract the maximum value of the vector to each of its elements before applying the operation wise expentional operator.\n",
    "\n",
    "## Problem setup\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "vec_len = 10\n",
    "block_size = 2\n",
    "\n",
    "data = np.random.random(vec_len)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07322196 0.06720894 0.12500862 0.12815786 0.106737   0.1044651\n",
      " 0.1384406  0.12197998 0.06843227 0.06634768]\n"
     ]
    }
   ],
   "source": [
    "data_small = data - np.max(data)\n",
    "numerator = np.exp(data_small)\n",
    "denominator = np.sum(numerator)\n",
    "custom_softmax = numerator/denominator\n",
    "assert np.allclose(custom_softmax, softmax(data))\n",
    "print(custom_softmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Direct implementation of the paper without vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 7.223314665650643\n",
      "m 0.8857019031149136\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "m = 0.\n",
    "d = 0.\n",
    "online_softmax = np.zeros_like(data)\n",
    "\n",
    "for j in data:\n",
    "    old_m = m\n",
    "    m = max(old_m, j)\n",
    "    d = d * math.exp(old_m-m) + math.exp(j-m)\n",
    "\n",
    "for index, j in enumerate(data):\n",
    "    online_softmax[index] = math.exp(j - m)/ d\n",
    "\n",
    "assert np.allclose(online_softmax, softmax(data))\n",
    "print(\"d\", d)\n",
    "print(\"m\", m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 7.223314665650642\n",
      "m 0.8857019031149136\n"
     ]
    }
   ],
   "source": [
    "block_max = -np.inf\n",
    "normalizer = 0.\n",
    "\n",
    "for block_start in range(0, vec_len, block_size):\n",
    "    block_end = block_start + block_size\n",
    "    block_data = data[block_start:block_end]\n",
    "    previous_block_max = block_max\n",
    "    block_max = max(np.max(block_data), block_max)\n",
    "    normalizer = normalizer * np.exp(previous_block_max - block_max) + np.sum(np.exp(block_data - block_max))\n",
    "\n",
    "assert block_max == np.max(data)\n",
    "assert np.allclose(normalizer, d), f\"{normalizer}, {d}\"\n",
    "print(\"d\",  normalizer)\n",
    "print(\"m\", block_max)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_max = -np.inf\n",
    "normalizer = 0.\n",
    "l = 0.\n",
    "softmax_result = np.zeros_like(data)\n",
    "\n",
    "for block_start in range(0, vec_len, block_size):\n",
    "    block_end = block_start + block_size\n",
    "\n",
    "    block_data = data[block_start:block_end]\n",
    "\n",
    "    old_max = data_max\n",
    "    block_data_max = np.max(block_data)\n",
    "    data_max = max(block_data_max, old_max)\n",
    "\n",
    "    block_data_f = np.exp(block_data - block_data_max)\n",
    "    block_data_l = np.sum(block_data_f)\n",
    "    block_data_normalizer = np.exp(block_data_max - data_max)\n",
    "    block_data_f_norm = block_data_f * block_data_normalizer\n",
    "\n",
    "    previous_block_data_normalizer = np.exp(old_max - data_max)\n",
    "\n",
    "    previous_l = l\n",
    "    l = previous_l * previous_block_data_normalizer + block_data_l * block_data_normalizer\n",
    "\n",
    "    softmax_result[block_start:block_end] = block_data_f_norm / l\n",
    "\n",
    "    # we fix the past by fixing both the numerator (previous_block_data_normalizer), removing the effect of the previous denominator and replace by the new one\n",
    "    softmax_result[:block_start] = softmax_result[:block_start] * previous_block_data_normalizer * previous_l / l\n",
    "\n",
    "assert np.allclose(softmax_result, softmax(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}