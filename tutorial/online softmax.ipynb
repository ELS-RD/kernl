{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Online softmax\n",
    "\n",
    "> **note**: this tutorial requires to be familiar with tiled `matmul`. A dedicated tutorial is available in `tutorial` folder of this repository.\n",
    "\n",
    "Naive implementation of `softmax` computation requires to perform several passes on the whole input vector.\n",
    "Vector loading from global memory (`GM`, aka the GPU DRAM) for each pass is by far the operation bottleneck (compared to the computation part).\n",
    "\n",
    "`softmax` `triton` tutorial avoid multiple read/write operations on `GM` by assuming that the whole input vector is small enough to be loaded in shared memory (`SRAM`).\n",
    "\n",
    "Below, we describe an approach when this assumption doesn't stand, aka when the vector is too large for the `SRAM`. In the case of `transformer` model, the `softmax` is applied to each row of a matrix of shape `(sequence length, sequence length)`, and the `SRAM` limit for an `fp16` vector is around 128 tokens.\n",
    "\n",
    "We will start the tutorial with a naive approach and optimize it.\n",
    "\n",
    "## Problem setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.random.manual_seed(456)\n",
    "\n",
    "nb_rows, nb_cols = 4, 16\n",
    "\n",
    "long_input_vec: torch.Tensor = torch.rand((nb_rows, nb_cols))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax computation\n",
    "\n",
    "### Safe softmax\n",
    "\n",
    "To avoid `FP16` or `FP32` overflow in `softmax` computation, it's usual to subtract to input vector its maximum value.\n",
    "This operation has no effect on the final output outside numerical stability.\n",
    "This is sometimes called `safe softmax` computation.\n",
    "\n",
    "### Memory bottleneck\n",
    "\n",
    "Computation of `safe softmax` on `PyTorch` requires multiple passes on the whole input vector if done manually:\n",
    "\n",
    "* one pass to find the maximum value\n",
    "* one pass to apply exponential operation to each value (numerator) and sum them (denominator)\n",
    "* one pass to perform the division `numerator / denominator`\n",
    "\n",
    "> because of the eager execution model, on `PyTorch` step 2 requires 2 passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input row max\n",
      " tensor([[0.9820],\n",
      "        [0.8412],\n",
      "        [0.9198],\n",
      "        [0.9778]])\n",
      "Below we reduce values amplitude, that's the safe part of safe softmax\n",
      "original 1st row input:\n",
      " tensor([0.6815, 0.0039, 0.7451, 0.7946, 0.6127, 0.6803, 0.9820, 0.0019, 0.1609,\n",
      "        0.5916, 0.6531, 0.8855, 0.7397, 0.0681, 0.3341, 0.3200]) safe softmax input 1st row:\n",
      " tensor([-0.3005, -0.9780, -0.2369, -0.1874, -0.3693, -0.3017,  0.0000, -0.9800,\n",
      "        -0.8211, -0.3904, -0.3289, -0.0965, -0.2423, -0.9139, -0.6479, -0.6620])\n"
     ]
    }
   ],
   "source": [
    "# torch softmax as a reference\n",
    "expected_softmax = torch.softmax(long_input_vec, dim=1)\n",
    "\n",
    "# 1st read, torch max output both indexes and values, we only want the values\n",
    "# we transpose it to get a vertical tensor\n",
    "row_max = torch.max(long_input_vec, dim=1).values[:, None]\n",
    "print(\"input row max\\n\", row_max)\n",
    "# 2nd read\n",
    "input_safe = long_input_vec - row_max\n",
    "print(\"Below we reduce values amplitude, that's the safe part of safe softmax\")\n",
    "print(\"original 1st row input:\\n\", long_input_vec[0,:], \"safe softmax input 1st row:\\n\", input_safe[0,:])\n",
    "\n",
    "softmax_numerator = torch.exp(input_safe)\n",
    "# 3rd read\n",
    "softmax_denominator = torch.sum(softmax_numerator, dim=1)[:, None]\n",
    "# 4th read\n",
    "naive_softmax = softmax_numerator / softmax_denominator\n",
    "\n",
    "assert torch.allclose(naive_softmax, expected_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Online softmax\n",
    "\n",
    "In their paper [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867.pdf), *M. Milakov & Al.* show an approach which makes parallelization possible by computing `softmax` progressively.\n",
    "Basically, we load the input vector in small blocks (adapted to the size of the `SRAM`) and compute 2 statistics in a single pass:\n",
    "\n",
    "* the maximum value\n",
    "* the denominator\n",
    "\n",
    "The achievement lies in the fact that you are supposed to know the maximum value of the vector to compute the denominator.\n",
    "At each step, our knowledge of the maximum value may evolve (we may meet a value bigger than our precedent maximum).\n",
    "When it happens, we just adjust the result of our computation of the precedent step.\n",
    "\n",
    "The adjustment procedure is based on rules of exponentiation: when multiplying a base raised to one exponent by the same base raised to another exponent, the exponents add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "online_softmax_simple = torch.zeros_like(long_input_vec)\n",
    "rows, cols = long_input_vec.shape\n",
    "\n",
    "for row in range(rows):\n",
    "    max_row = 0.\n",
    "    softmax_denominator = 0.\n",
    "    for col in range(cols):  # scalar level iteration\n",
    "        val = long_input_vec[row, col]\n",
    "        old_max_row = max_row\n",
    "        max_row = max(old_max_row, val)\n",
    "        # np.exp(old_max_row - max_row) is the adjustment factor of our precedent softmax_denominator,\n",
    "        # after this multiplication it's like we had substracted max_row to all values instead of old_max_row\n",
    "        softmax_denominator = softmax_denominator * torch.exp(old_max_row - max_row) + torch.exp(val - max_row)\n",
    "\n",
    "    # leverage our 2 statistics\n",
    "    online_softmax_simple[row, :] = torch.exp(long_input_vec[row, :] - max_row) / softmax_denominator\n",
    "\n",
    "assert torch.allclose(online_softmax_simple, expected_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The very same procedure can be extended to manage blocks without any formula change, instead of moving one scalar at a time you just take a whole vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}