{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:08.670920Z",
     "iopub.status.busy": "2022-10-09T10:17:08.670500Z",
     "iopub.status.idle": "2022-10-09T10:17:10.879953Z",
     "shell.execute_reply": "2022-10-09T10:17:10.878323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: tokenizer in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: sentencepiece in /home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages (0.1.97)\r\n",
      "Wed Oct 12 09:00:17 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:03:00.0  On |                  N/A |\r\n",
      "| 36%   44C    P5    45W / 350W |    313MiB / 24576MiB |     29%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1656      G   /usr/lib/xorg/Xorg                129MiB |\r\n",
      "|    0   N/A  N/A      7959      G   /usr/bin/gnome-shell               40MiB |\r\n",
      "|    0   N/A  N/A      9740      G   ...on/Bin/AgentConnectix.bin        4MiB |\r\n",
      "|    0   N/A  N/A     11452      G   ...RendererForSitePerProcess        4MiB |\r\n",
      "|    0   N/A  N/A     88529      G   ...veSuggestionsOnlyOnDemand       60MiB |\r\n",
      "|    0   N/A  N/A    244483      G   ...688074910081220309,131072       71MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizer sentencepiece\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:10.887206Z",
     "iopub.status.busy": "2022-10-09T10:17:10.886686Z",
     "iopub.status.idle": "2022-10-09T10:17:12.410746Z",
     "shell.execute_reply": "2022-10-09T10:17:12.409928Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import time\n",
    "import torchdynamo\n",
    "import torch\n",
    "from typing import List\n",
    "from kernl.optimizer.dynamo_backend import dynamo_backend_ofi\n",
    "from kernl.implementations.cuda_graph import cuda_graphs_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/kernl/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default cache size needs to be increased to store the many graphs with generative models\n",
    "torchdynamo.config.cache_size_limit = 512\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"Translate in English: c'est beaucoup plus rapide avec ces optimisations !\",\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    ").to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469151020050049\n",
      "c'est beaucoup plus rapide avec ces optimisations!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    for _ in range(3):\n",
    "        output = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=100,\n",
    "            max_length=100,\n",
    "        )\n",
    "    start = time.time()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T10:17:19.111208Z",
     "iopub.status.busy": "2022-10-09T10:17:19.110747Z",
     "iopub.status.idle": "2022-10-09T10:17:19.117920Z",
     "shell.execute_reply": "2022-10-09T10:17:19.116830Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchdynamo.optimizations import BACKENDS\n",
    "\n",
    "model.encoder.forward2 = model.encoder.forward\n",
    "model.decoder.forward2 = model.decoder.forward\n",
    "\n",
    "\n",
    "def compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    compiled = BACKENDS[\"cudagraphs\"](gm, example_inputs)\n",
    "    return compiled\n",
    "\n",
    "\n",
    "def run_encoder(*args, **kwargs):\n",
    "    with torchdynamo.optimize(compiler):\n",
    "        return model.encoder.forward2(*args, **kwargs)\n",
    "\n",
    "\n",
    "def run_decoder(*args, **kwargs):\n",
    "    with torchdynamo.optimize(compiler):\n",
    "        return model.decoder.forward2(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    out1 = run_decoder(**input_ids)\n",
    "    out2 = model.decoder(**input_ids)\n",
    "    assert torch.allclose(out1.last_hidden_state, out2.last_hidden_state, atol=1e-1)\n",
    "    out3 = run_encoder(**input_ids)\n",
    "    out4 = model.encoder(**input_ids)\n",
    "    assert torch.allclose(out3.last_hidden_state, out4.last_hidden_state, atol=1e-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model.encoder.forward = run_encoder\n",
    "model.decoder.forward = run_decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341.7478229999542\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.178056001663208\n",
      "c'est beaucoup plus rapide avec ces optimisations!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=100,\n",
    "        max_length=100,\n",
    "    )\n",
    "    print(time.time() - start)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "047edc3d04374c528f27024bb17a654b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a40339901e3d4b4ba40b54d047ba8ea9",
       "placeholder": "​",
       "style": "IPY_MODEL_6c2241efc65345ffa6b32847eec24ae9",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "61727d1224ca498fa3d81b8456047c8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c2241efc65345ffa6b32847eec24ae9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ffd37b7da934584a97f27a143531df6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a40339901e3d4b4ba40b54d047ba8ea9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab31f1f54ef9429783d03a05e906517e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_047edc3d04374c528f27024bb17a654b",
        "IPY_MODEL_de91c477767649a59ab0df6e3255ff61",
        "IPY_MODEL_b84e493dfa184888b745c8cafd2a40aa"
       ],
       "layout": "IPY_MODEL_61727d1224ca498fa3d81b8456047c8d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ad7ce67b0f1a4c0683d3635fdac61575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b34763a9b06a4e178f5cea545f118988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b84e493dfa184888b745c8cafd2a40aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ad7ce67b0f1a4c0683d3635fdac61575",
       "placeholder": "​",
       "style": "IPY_MODEL_f44c8b256f0747f9b277d56f39049264",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:00&lt;00:00, 73.34it/s]"
      }
     },
     "de91c477767649a59ab0df6e3255ff61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b34763a9b06a4e178f5cea545f118988",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ffd37b7da934584a97f27a143531df6",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "f44c8b256f0747f9b277d56f39049264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
