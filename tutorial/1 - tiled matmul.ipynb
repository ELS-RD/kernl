{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tiled matmul\n",
    "\n",
    "In this simple tutorial we will see how to perform a `matmul` with tiling.\n",
    "Tiling is a technique based on matrix partition, each block is called a tile.\n",
    "\n",
    "With tiling, `matmul`:\n",
    "* computation can be performed in parallel, a domain where GPUs excels;\n",
    "* global memory (GM) access are limited, GM access being the GPU bottleneck (compared to computation).\n",
    "\n",
    "## GEMM introduction\n",
    "\n",
    "Below we define the problem size and initialize the matrices.\n",
    "In `GEMM` a problem is defined by 3 numbers: `KMN`.\n",
    "\n",
    "`D = α * A * B + β * C` with:\n",
    "* `D` shape is `MxN`\n",
    "* `A` shape is `MxK`\n",
    "* `B` shape is `KxN`\n",
    "* `C` shape is `MxN`\n",
    "* `α` and `β` are 2 constants\n",
    "\n",
    "> for readability, below `α` is implicitly set to 1 and `β` to 0 so `C` do not appear.\n",
    "> Obviously, re-introducing them would be very easy.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "M, N, K = 15, 9, 12\n",
    "\n",
    "A = torch.rand((M, K))\n",
    "B = torch.rand((K, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple matmul with tiling\n",
    "\n",
    "Simple example showing how we can perform a `matmul` through tiling.\n",
    "\n",
    "Basic introduction to the subject can be found here:\n",
    "\n",
    "* https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\n",
    "* https://penny-xu.github.io/blog/tiled-matrix-multiplication\n",
    "\n",
    "Parallelization can be applied at each `M` and `N` for loop levels.\n",
    "However, best use of global memory access requires to be a bit smarter.\n",
    "Check our dedicated explanation in tutorials.\n",
    "\n",
    "Values used below are a arbitrary and small to be printable if needed.\n",
    "Rule of thumb in defining tile shape is:\n",
    "* large tile size increase data reuse, but decrease thread-level parallelism;\n",
    "* small tile size increase thread-level parallelism but reduce data reuse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# for simplification tile shapes are all multiple of matrix shapes\n",
    "# otherwise we would need to check matrix bounds and mask out of bounds values by 0s in tiles\n",
    "block_M, block_N, block_K = M // 3, N // 3, K // 3\n",
    "\n",
    "output = torch.zeros((M, N))\n",
    "for index_M in range(0, M, block_M):\n",
    "    start_M = index_M\n",
    "    end_M = index_M + block_M\n",
    "\n",
    "    for index_N in range(0, N, block_N):\n",
    "        start_N = index_N\n",
    "        end_N = index_N + block_N\n",
    "        accumulator = torch.zeros((block_M, block_N))\n",
    "        for index_K in range(0, K, block_K):\n",
    "            start_K = index_K\n",
    "            end_K = index_K + block_K\n",
    "\n",
    "            tile_A = A[start_M:end_M, start_K:end_K]\n",
    "            tile_B = B[start_K:end_K, start_N:end_N]\n",
    "            # @ means matmul in numpy and pytorch\n",
    "            accumulator += tile_A @ tile_B\n",
    "        output[start_M:end_M, start_N:end_N] = accumulator\n",
    "\n",
    "assert torch.allclose(output, A @ B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}