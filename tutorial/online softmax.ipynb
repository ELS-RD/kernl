{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Online softmax and flash attention\n",
    "\n",
    "> **note**: this tutorial requires to be familiar with tiled `matmul`. A dedicated tutorial is available in `tutorial` folder of this repository.\n",
    "\n",
    "Naive implementation of `softmax` computation requires to perform several passes on the whole input vector.\n",
    "Vector loading from global memory (`GM`, aka the GPU DRAM) for each pass is by far the operation bottleneck (compared to the computation part).\n",
    "\n",
    "`softmax` `triton` tutorial avoid multiple read/write operations on `GM` by assuming that the whole vector is small enough to be loaded in shared memory (`SRAM`).\n",
    "\n",
    "Below, we describe an approach when this assumption doesn't stand, aka when the vector is too large for the `SRAM`. In the case of `transformer` model, the `softmax` is applied to each row of a matrix of shape `(sequence length, sequence length)`, and the `SRAM` limit for an `fp16` vector is around 128 tokens.\n",
    "\n",
    " Moreover, we will see a second trick used in [`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`](https://arxiv.org/pdf/2205.14135.pdf) paper to fuse the whole attention process in a single kernel.\n",
    "\n",
    "> for simplicity, we will compute the attention after the `KV` `matmul` as there is nothing special about this operation.\n",
    "\n",
    "We will start the tutorial with a naive approach and optimize it until we reach the `flash attention` attention computation approach.\n",
    "\n",
    "## Problem setup\n",
    "\n",
    "We name axis as usually done in GEMM: `M`, `N` and `K`.\n",
    "For more information about `GEMM`, check dedicated tutorial and following documennt: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "M, N, K = 12, 4, 16  # for simplification M and K are multiples of N\n",
    "block_size_M, block_size_K = N, N  # tile size for matmul, no op bigger than this size can be stored in SRAM\n",
    "\n",
    "long_input_vec = np.random.random((M, K))\n",
    "small_vec = np.random.random((K, N))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax computation\n",
    "\n",
    "### Safe softmax\n",
    "\n",
    "To avoid `FP16` or `FP32` overflow in `softmax` computation, it's usual to subtract to input vector its maximum value.\n",
    "This operation has no effect on the final output outside numerical stability.\n",
    "This is sometimes called `safe softmax` computation.\n",
    "\n",
    "### Memory bottleneck\n",
    "\n",
    "Computation of `safe softmax` on `PyTorch` requires multiple passes on the whole input vector if done manually:\n",
    "\n",
    "* one pass to find the maximum value\n",
    "* one pass to apply exponential operation to each value (numerator) and sum them (denominator)\n",
    "* one pass to perform the division `numerator / denominator`\n",
    "\n",
    "> because of the eager execution model, on `PyTorch` step 2 requires 2 passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expected_softmax = softmax(long_input_vec, axis=1)\n",
    "expected_attention = expected_softmax @ small_vec\n",
    "\n",
    "# 1st read\n",
    "row_max = np.max(long_input_vec, axis=1)[:, None]\n",
    "# 2nd read\n",
    "input_safe = long_input_vec - row_max\n",
    "softmax_numerator = np.exp(input_safe)\n",
    "# 3rd read\n",
    "softmax_denominator = np.sum(softmax_numerator, axis=1)[:, None]\n",
    "# 4th read\n",
    "naive_softmax = softmax_numerator / softmax_denominator\n",
    "# final matmul (another read / write)\n",
    "matmul_result = naive_softmax @ small_vec\n",
    "\n",
    "assert np.allclose(naive_softmax, expected_softmax)\n",
    "assert np.allclose(matmul_result, expected_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Online softmax\n",
    "\n",
    "In their paper [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867.pdf), *M. Milakov & Al.* show an approach which makes parallelization possible by computing `softmax` progressively.\n",
    "Basically, we load the input vector in small blocks (adapted to the size of the `SRAM`) and compute 2 statistics in a single pass:\n",
    "\n",
    "* the maximum value\n",
    "* the denominator\n",
    "\n",
    "The achievement lies in the fact that you are supposed to know the maximum value of the vector to compute the denominator.\n",
    "At each step, our knowledge of the maximum value may evolve (we may meet a value bigger than our precedent maximum).\n",
    "When it happens, we just adjust the result of our computation of the precedent step.\n",
    "\n",
    "The adjustment procedure is based on rules of exponentiation: when multiplying a base raised to one exponent by the same base raised to another exponent, the exponents add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "online_softmax_simple = np.zeros_like(long_input_vec)\n",
    "rows, cols = long_input_vec.shape\n",
    "\n",
    "for row in range(rows):\n",
    "    max_row = 0.\n",
    "    softmax_denominator = 0.\n",
    "    for col in range(cols):\n",
    "        val = long_input_vec[row, col]\n",
    "        old_max_row = max_row\n",
    "        max_row = max(old_max_row, val)\n",
    "        # np.exp(old_max_row - max_row) is the adjustment factor of our precedent softmax_denominator,\n",
    "        # after this multiplication it's like we had substracted max_row to all values instead of old_max_row\n",
    "        softmax_denominator = softmax_denominator * np.exp(old_max_row - max_row) + np.exp(val - max_row)\n",
    "\n",
    "    # leverage our 2 statistics\n",
    "    online_softmax_simple[row, :] = np.exp(long_input_vec[row, :] - max_row) / softmax_denominator\n",
    "\n",
    "assert np.allclose(online_softmax_simple, expected_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Flash attention trick\n",
    "\n",
    "`Online softmax` limits the computation to 2 passes.\n",
    "In [`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`](https://arxiv.org/pdf/2205.14135.pdf), there is only one pass, the `softmax` itself is never materialized, which is the key to the low memory footprint.\n",
    "\n",
    "The trick is that in attention mechanism, the `softmax` is followed by a `matmul` with the `V` matrix.\n",
    "`V` matrix column number is low per head (<= 128 even for GPT-3 size model), and so the attention output will have few columns too. As explained in fused matmul tutorial in this repo, this offers the possibility to never materialize intermediate results, in our case we can avoid materializing the `softmax`.\n",
    "\n",
    "To not store the whole softmax vectors in memory, we will progressively convert input data (`KV` `matmul` in theory, not shown here), compute the output of the `matmul` with the information we have, and perform some adjustments depending on what we learn during each input data loading like in `online softmax` above. We leverage the fact that multiplication is associative and commutative and adjust the `matmul` output instead of the `softmax` itself because the last one is never materialized, that is the main difference with the procedure above. That's the trick.\n",
    "\n",
    "> lines below refer to `Algorithm 1` from [flatsh attention](https://arxiv.org/pdf/2205.14135.pdf) paper, it's the attention for a single head. As written above we omit the `KV` `matmul` for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# those global variables represent the global memory\n",
    "# they are the only ones bigger than what the SRAM can store\n",
    "online_softmax = np.zeros_like(long_input_vec)\n",
    "online_attention = np.zeros((M, N))\n",
    "\n",
    "for block_start_M in range(0, M, block_size_M):\n",
    "    block_end_M = block_start_M + block_size_M\n",
    "    # init some variables required at the row level\n",
    "    # line 4, mi will store the row max (computed progressively, block after block)\n",
    "    mi = np.full((block_size_M, 1), -np.inf)\n",
    "    # line 4, li will store the denominator of the softmax\n",
    "    li = np.zeros((block_size_M, 1))\n",
    "    # load from global memory, Oi contains the matmum result\n",
    "    Oi = online_attention[block_start_M:block_end_M, :]\n",
    "    for block_start_K in range(0, K, block_size_K):\n",
    "        block_end_K = block_start_K + block_size_K\n",
    "        # load a block from input tensor where we apply softmax on each line\n",
    "        block = long_input_vec[block_start_M:block_end_M, block_start_K:block_end_K]\n",
    "        # line 6, load a block from matmul input tensor\n",
    "        Vj = small_vec[block_start_K:block_end_K, :]\n",
    "        # line 10, find max of each row of the current loaded block (and only this block)\n",
    "        mij_hat = np.max(block, axis=1)[:, None]\n",
    "        # line 10, compute the softmax numerator like if we only had the data from this block (and nothing before or after)\n",
    "        pij_hat = np.exp(block - mij_hat)\n",
    "        # line 10, compute the softmax denominator like if we only had the data from this block (and nothing before or after)\n",
    "        lij_hat = np.sum(pij_hat, axis=1)[:, None]\n",
    "\n",
    "        # line 11, find max of each row regarding the current block and all the previous ones we have already visited\n",
    "        mi_new = np.max(np.column_stack([mi, mij_hat]), axis=1)[:, None]\n",
    "\n",
    "        # line 11, adjusting factor (see online softmax computation above) leveraging the rule of exponentiation\n",
    "        li_new = np.exp(mi - mi_new) * li + np.exp(mij_hat - mi_new) * lij_hat\n",
    "\n",
    "        # line 12, first part before the \"+\" is the adjustment of the past blocks\n",
    "        # second part after the \"+\" is the incorporation of the information from the current block and the matmul for this block\n",
    "        Oi = (li * np.exp(mi - mi_new) * Oi / li_new) + (np.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj\n",
    "\n",
    "        # line 13, save statistics\n",
    "        mi = mi_new  # row max\n",
    "        li = li_new  # softmax denominator\n",
    "    # save attention block to global memory\n",
    "    online_attention[block_start_M:block_end_M, :] = Oi\n",
    "\n",
    "\n",
    "assert np.allclose(online_attention, expected_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}