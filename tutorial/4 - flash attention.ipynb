{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Flash attention (forward)\n",
    "\n",
    "> This notebook is based on knowledge from fused matmul and online softmax notebooks.\n",
    "\n",
    "As seen in its dedicated notebook, `online softmax` limits the computation to 2 passes on global memory input.\n",
    "\n",
    "In [`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`](https://arxiv.org/pdf/2205.14135.pdf), the goal of the author is to have only one pass on the data. For that purpose, the `softmax` itself only exists in SRAM and is never saved to `GPU` global memory, which is the key to the low memory footprint of the approach.\n",
    "\n",
    "The trick is that in attention mechanism, the `softmax` is followed by a `matmul` with the `V` matrix.\n",
    "`V` matrix column number (`d` in the paper, aka the number of dimensions per head for the model) is low (<= 128 even for a `GPT-3` size model), and so the attention output will have few columns too. This offers the possibility to never save to global memory intermediate results, in our case we can avoid materializing the `softmax`.\n",
    "\n",
    "One key trick to this process which is slightly different from original `online softmax` paper is related to the adjustment of the regularizer term. In the original implementation, when you load new part of the input vector, you may discover a new row maximum value, if so, you need to adjust already loaded input data in a way which makes them like you had applied the new row maximum since the begining of the computation. This means you need to have saved somewhere those already loaded input data to adjust them. But, as you know, we don't want to save those very large data. So, the trick is the following: the adjustment is a multiplication of the past data by a scalar, multiplication is associative and commutative, so we can apply the adjustment not on `softmax` output itself, but to the output of the second `matmul` (`softmax` output times `V` matrix). This output is the whole attention output and is saved to global memory.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.random.manual_seed(456)\n",
    "\n",
    "N, d = 16, 8\n",
    "\n",
    "Q_mat = torch.rand((N, d))\n",
    "K_mat = torch.rand((N, d))\n",
    "V_mat = torch.rand((N, d))\n",
    "\n",
    "# tile size for matmul, no op bigger than this size can be stored in SRAM\n",
    "Br = 4\n",
    "Bc = d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classic PyTorch implementation of attention\n",
    "\n",
    "We start by implementing attention mechanism in PyTorch.\n",
    "The code is simple and many read/write in global memory are done."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "expected_softmax = torch.softmax(Q_mat @ K_mat.T, dim=1)\n",
    "expected_attention = expected_softmax @ V_mat\n",
    "\n",
    "# 1st read\n",
    "S_mat = Q_mat @ K_mat.T\n",
    "row_max = torch.max(S_mat, dim=1).values[:, None]\n",
    "# 2nd read\n",
    "input_safe = S_mat - row_max\n",
    "softmax_numerator = torch.exp(input_safe)\n",
    "# 3rd read\n",
    "softmax_denominator = torch.sum(softmax_numerator, dim=1)[:, None]\n",
    "# 4th read\n",
    "naive_softmax = softmax_numerator / softmax_denominator\n",
    "# final matmul (another read / write)\n",
    "matmul_result = naive_softmax @ V_mat\n",
    "\n",
    "assert torch.allclose(naive_softmax, expected_softmax)\n",
    "assert torch.allclose(matmul_result, expected_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple tiled matmul\n",
    "\n",
    "We start with the `matmul` of `QK^t`.\n",
    "We will do it with tiling.\n",
    "One particularity of transformer models is that the number of columns (`d`) of those matrices is low and several complete rows can be stored in shared memory, so we won't have to iterate across this axis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "S_mat_for_check = torch.zeros((N, N))\n",
    "\n",
    "for block_start_Bc in range(0, N, Bc):\n",
    "    block_end_Bc = block_start_Bc + Bc\n",
    "    Kj = K_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d\n",
    "    for block_start_Br in range(0, N, Br):\n",
    "        block_end_Br = block_start_Br + Br\n",
    "        Qi = Q_mat[block_start_Br:block_end_Br, :]  # shape Br x d\n",
    "\n",
    "        # QKt at the tile level\n",
    "        Sij = Qi @ Kj.T  # shape Br x Bc\n",
    "        S_mat_for_check[block_start_Br:block_end_Br, block_start_Bc:block_end_Bc] += Sij\n",
    "\n",
    "assert torch.allclose(S_mat_for_check, Q_mat @ K_mat.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fused `matmul`\n",
    "\n",
    "We will perform the computation `O=SV^t` where `S=QK^t`.\n",
    "For now, we do not put any `softmax` in between.\n",
    "\n",
    "Our main challenge is to build on top of the previous notebook block and not materialize (not save in `GPU` global memory) the intermediate `matmul` output `S`. One trick to reduce global memory accesses is to reuse data as much as possible. That is why we start iterating on columns and load 2 blocks (`Kj` and `Vj`) and reuse them during the iteration on rows. The input matrices are supposed to not be transposed.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "O = torch.zeros((N, d))\n",
    "\n",
    "for block_start_Bc in range(0, N, Bc):\n",
    "    block_end_Bc = block_start_Bc + Bc\n",
    "    Kj = K_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d\n",
    "    Vj = V_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d\n",
    "    for block_start_Br in range(0, N, Br):\n",
    "        block_end_Br = block_start_Br + Br\n",
    "        Qi = Q_mat[block_start_Br:block_end_Br, :]  # shape Br x d\n",
    "\n",
    "        # QKt at the tile level\n",
    "        Sij = Qi @ Kj.T  # shape Br x Bc\n",
    "        Oi = Sij @ Vj  # shape Br x d\n",
    "        O[block_start_Br:block_end_Br, :] += Oi\n",
    "\n",
    "assert torch.allclose(O, (Q_mat @ K_mat.T) @ V_mat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full flash attention (forward)\n",
    "\n",
    "In the previous block, we didn't apply the `softmax` on top of `QK^t` `matmul`.\n",
    "The challenge in introducing it is to not materialize its output (which would have the same shape as `QK^t`, aka `seq len X seq len`).\n",
    "\n",
    "For that purpose, we will leverage the `online softmax` technique presented in details in a dedicated notebook.\n",
    "The idea is that to compute `safe softmax` we need to know the row `max` (and the `softmax` denominator which itself depends on this row `max` statistic), an information that we can only know by scanning whole row data. `online softmax` computes `softmax` progressively, without needing complete pass on data first, block of row after block of row. During the process, each time we discover in a block a `max` bigger than the currently known row `max`, we correct the already computed values in a way which simulates that we have applied the new row `max` for the softmax numerator and denominator since the beginning. The correction (of the softmax denominator) is applied here:\n",
    "\n",
    "```python\n",
    "li_new = torch.exp(mi - mi_new) * li + torch.exp(mij_hat - mi_new) * lij_hat\n",
    "```\n",
    "\n",
    "> This line is exactly the same mechanism seen in `online softmax` notebook but applied to a vector instead of scalar (the math stays the same).\n",
    "\n",
    "The remaining question is where to correct the *past*? Indeed, we want to perform all computations in a single pass, so we do not save first `matmul` output to `GPU` global memory, meaning there is no `past` part of first `matmul` output to adjust.\n",
    "\n",
    "However, we can apply the correction to the matrix `O` (output of the second `matmul`), it works because multiplication by a scalar is commutative, aka we can change the order of operations and get the same result mathematically (this is not 100% true in our code, as float numbers has limited precision and introduces some roundings, still, the effect is small and Okish in deep learning). This is done in the line:\n",
    "\n",
    "```python\n",
    "Oi = (li * torch.exp(mi - mi_new) * Oi / li_new) + (torch.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj\n",
    "```\n",
    "\n",
    "In the left part of the addition (`(li * torch.exp(mi - mi_new) * Oi / li_new)`), `Oi` contains the sum of past output tiles, and that's where we can correct the past. In the right part of the addition (`(torch.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj`), we first correct the current tile `softmax` and then the current tile output.\n",
    "\n",
    "> lines below refer to `Algorithm 1` from [flash attention](https://arxiv.org/pdf/2205.14135.pdf) paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# variables outside the for loop represent the global memory\n",
    "# they are the only ones bigger than what the SRAM can store\n",
    "O = torch.zeros((N, d))\n",
    "\n",
    "# For the 2 variables below, they may be removed in a serially executed code (in particular the outter for loop)\n",
    "# They are needed in parallelized execution where each thread block need to sync its findings with the others\n",
    "# line 4, l will store the denominator of the softmax for each row\n",
    "l = torch.zeros((N, 1))\n",
    "# line 4, m will store the row max (computed progressively, block after block)\n",
    "m = torch.full((N, 1), -torch.inf)\n",
    "\n",
    "for block_start_Bc in range(0, N, Bc):\n",
    "    block_end_Bc = block_start_Bc + Bc\n",
    "    # line 6, load a block from matmul input tensor\n",
    "    Kj = K_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d\n",
    "    Vj = V_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d\n",
    "    for block_start_Br in range(0, N, Br):\n",
    "        block_end_Br = block_start_Br + Br\n",
    "\n",
    "        # line 8, load stuff from globabl memory, aka the work of the other thread blocks\n",
    "        mi = m[block_start_Br:block_end_Br, :]  # shape Br x 1\n",
    "        li = l[block_start_Br:block_end_Br, :]  # shape Br x 1\n",
    "        Oi = O[block_start_Br:block_end_Br, :]  # shape Br x d\n",
    "        Qi = Q_mat[block_start_Br:block_end_Br, :]  # shape Br x d\n",
    "\n",
    "        # line 9, QKt at the tile level\n",
    "        Sij = Qi @ Kj.T  # shape Br x Bc\n",
    "\n",
    "        # line 10, find max of each row of the current loaded block (and only this block)\n",
    "        mij_hat = torch.max(Sij, dim=1).values[:, None]\n",
    "        # line 10, compute the softmax numerator like if we only had the data from this block (and nothing before or after)\n",
    "        pij_hat = torch.exp(Sij - mij_hat)\n",
    "        # line 10, compute the softmax denominator like if we only had the data from this block (and nothing before or after)\n",
    "        lij_hat = torch.sum(pij_hat, dim=1)[:, None]\n",
    "\n",
    "        # line 11, find max of each row regarding the current block and all the previous ones we have already visited\n",
    "        mi_new = torch.max(torch.column_stack([mi, mij_hat]), dim=1).values[:, None]\n",
    "        # line 11, adjusting factor (see online softmax computation above) leveraging the rule of exponentiation\n",
    "        li_new = torch.exp(mi - mi_new) * li + torch.exp(mij_hat - mi_new) * lij_hat\n",
    "\n",
    "        # line 12, first part before the \"+\" is the adjustment of the past blocks\n",
    "        # second part after the \"+\" is the incorporation of the information from the current block and the matmul for this block\n",
    "        Oi = (li * torch.exp(mi - mi_new) * Oi / li_new) + (torch.exp(mij_hat - mi_new) * pij_hat / li_new) @ Vj\n",
    "\n",
    "        # Note that we replace (=) and not update (+=) the global variables like we would do in tilted matmul\n",
    "        # line 13, save statistics\n",
    "        m[block_start_Br:block_end_Br, :] = mi_new  # row max\n",
    "        l[block_start_Br:block_end_Br, :] = li_new  # softmax denominator\n",
    "        # save attention block to global memory\n",
    "        O[block_start_Br:block_end_Br, :] = Oi\n",
    "\n",
    "assert torch.allclose(O, expected_attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Triton implementation and original Flash attention Cuda implementations differ on an important point.\n",
    "\n",
    "In Cuda implementation, algorithm above is executed in a serialized way. The parallelization only happens at the head x batch level (so you need on `A100` at least head x batch >= 80 to keep the GPU busy).\n",
    "\n",
    "In Triton, the inner and outer loops above are switched and the parallelization happens at the level of the outer loop, it makes parallelization works even for small batches. See https://github.com/HazyResearch/flash-attention/issues/40 for detailed analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}